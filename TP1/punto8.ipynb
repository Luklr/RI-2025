{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import re\n",
    "from punto5 import Tokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms(data: list) -> dict:\n",
    "    data.sort()\n",
    "    unique = []\n",
    "    count = 0\n",
    "    last_word = \"\"\n",
    "    for word in data:\n",
    "        if word != last_word:\n",
    "            if last_word != \"\":\n",
    "                unique.append({\"term\": last_word, \"tf\": count})\n",
    "            count = 1\n",
    "            last_word = word\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "    if last_word != \"\":\n",
    "        unique.append({\"term\": last_word, \"tf\": count})\n",
    "        return unique\n",
    "\n",
    "\n",
    "def term_in_data(data: list, term: str) -> bool:\n",
    "    for element in data:\n",
    "        if element[\"term\"] == term:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def set_term(data: list, term: str, tf: int) -> list:\n",
    "    for element in data:\n",
    "        if element[\"term\"] == term:\n",
    "            element[\"df\"] += 1\n",
    "            element[\"tf\"] += tf\n",
    "            return data\n",
    "\n",
    "def read_file(url):\n",
    "    data = []\n",
    "    tokenizer = Tokenizer(words=True, names=True, abbreviations=True, numbers=True)\n",
    "    with open(url, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            terms_data = terms(tokens)\n",
    "            for term in terms_data:\n",
    "                if not term_in_data(data,term[\"term\"]):\n",
    "                    data.append({\n",
    "                        \"term\": term[\"term\"],\n",
    "                        \"df\": 1,\n",
    "                        \"tf\": term[\"tf\"]\n",
    "                    })\n",
    "                else:\n",
    "                    set_term(data, term[\"term\"], term[\"tf\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"pg2000.txt\"\n",
    "data = read_file(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poda de palabras mas frecuentes: \n",
      "Porcentaje de terminos podados que eran stopwords (10%): 0.07\n",
      "Porcentaje de terminos podados que eran stopwords (20%): 0.04\n",
      "Porcentaje de terminos podados que eran stopwords (30%): 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lukfi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "data.sort(key=lambda x: x[\"tf\"], reverse=True)\n",
    "n_terms = len(data)\n",
    "\n",
    "# podo el 10, 20 y 30% de los datos mas frecuentes\n",
    "pruned_data_10pct = data[int(n_terms*0.1):]\n",
    "pruned_data_20pct = data[int(n_terms*0.2):]\n",
    "pruned_data_30pct = data[int(n_terms*0.3):]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "# verifico la cantidad de palabras podadas que son stopwords\n",
    "# y guardo aquellas que no lo son\n",
    "data_10pct = set([term[\"term\"] for term in data]) - set([term[\"term\"] for term in pruned_data_10pct])\n",
    "data_20pct = set([term[\"term\"] for term in data]) - set([term[\"term\"] for term in pruned_data_20pct])\n",
    "data_30pct = set([term[\"term\"] for term in data]) - set([term[\"term\"] for term in pruned_data_30pct])\n",
    "\n",
    "total_stopwords_10pct = 0\n",
    "non_stopwords_terms_10pct = []\n",
    "for term in data_10pct:\n",
    "    if term in stop_words:\n",
    "        total_stopwords_10pct+=1\n",
    "    else:\n",
    "        non_stopwords_terms_10pct.append(term)\n",
    "\n",
    "total_stopwords_20pct = 0\n",
    "non_stopwords_terms_20pct = []\n",
    "for term in data_20pct:\n",
    "    if term in stop_words:\n",
    "        total_stopwords_20pct+=1\n",
    "    else:\n",
    "        non_stopwords_terms_20pct.append(term)\n",
    "\n",
    "total_stopwords_30pct = 0\n",
    "non_stopwords_terms_30pct = []\n",
    "for term in data_30pct:\n",
    "    if term in stop_words:\n",
    "        total_stopwords_30pct+=1\n",
    "    else:\n",
    "        non_stopwords_terms_30pct.append(term)\n",
    "\n",
    "print(f\"Poda de palabras mas frecuentes: \")\n",
    "print(f\"Porcentaje de terminos podados que eran stopwords (10%): {(total_stopwords_10pct / len(data_10pct)):.2f}\")\n",
    "print(f\"Porcentaje de terminos podados que eran stopwords (20%): {(total_stopwords_20pct / len(data_20pct)):.2f}\")\n",
    "print(f\"Porcentaje de terminos podados que eran stopwords (30%): {(total_stopwords_30pct / len(data_30pct)):.2f}\")\n",
    "\n",
    "with open(\"punto8_terminos_podados_10pct.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(non_stopwords_terms_10pct))\n",
    "with open(\"punto8_terminos_podados_20pct.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(non_stopwords_terms_20pct))\n",
    "with open(\"punto8_terminos_podados_30pct.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(non_stopwords_terms_30pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de palabras calculadas en el 10% del vocabulario: 608334.0334863396\n",
      "Cantidad de palabras calculadas en el 20% del vocabulario: 626053.85833007\n",
      "Cantidad de palabras calculadas en el 30% del vocabulario: 635301.657369608\n",
      "Cantidad de palabras reales en el 10% del vocabulario: 312349\n",
      "Cantidad de palabras reales en el 20% del vocabulario: 331791\n",
      "Cantidad de palabras reales en el 30% del vocabulario: 341398\n"
     ]
    }
   ],
   "source": [
    "data.sort(key=lambda x: x[\"tf\"], reverse=True)\n",
    "ranks = np.arange(1, len(data) + 1)\n",
    "freqs = np.array([entry[\"tf\"] for entry in data])\n",
    "\n",
    "# Ajuste con Zipf usando regresión lineal en escala logarítmica\n",
    "log_ranks = np.log(ranks)\n",
    "log_freqs = np.log(freqs)\n",
    "coeffs = np.polyfit(log_ranks, log_freqs, 1)  # Ajuste lineal en log-log\n",
    "\n",
    "alfa = coeffs[0]\n",
    "c = np.exp(coeffs[1])\n",
    "\n",
    "n_terms = len(data)\n",
    "\n",
    "n_tokens_10pct = 0\n",
    "n_tokens_20pct = 0\n",
    "n_tokens_30pct = 0\n",
    "for i in range(1, int(n_terms * 0.1) + 1):\n",
    "    n_tokens_10pct += c * i ** alfa\n",
    "for i in range(1, int(n_terms * 0.2) + 1):\n",
    "    n_tokens_20pct += c * i ** alfa\n",
    "for i in range(1, int(n_terms * 0.3) + 1):\n",
    "    n_tokens_30pct += c * i ** alfa\n",
    "\n",
    "# frecuencias reales\n",
    "n_tokens_10pct_real = 0\n",
    "n_tokens_20pct_real = 0\n",
    "n_tokens_30pct_real = 0\n",
    "for i in range(1, int(n_terms * 0.1) + 1):\n",
    "    n_tokens_10pct_real += freqs[i]\n",
    "for i in range(1, int(n_terms * 0.2) + 1):\n",
    "    n_tokens_20pct_real += freqs[i]\n",
    "for i in range(1, int(n_terms * 0.3) + 1):\n",
    "    n_tokens_30pct_real += freqs[i]\n",
    "\n",
    "print(f\"Cantidad de palabras calculadas en el 10% del vocabulario: {n_tokens_10pct}\")\n",
    "print(f\"Cantidad de palabras calculadas en el 20% del vocabulario: {n_tokens_20pct}\")\n",
    "print(f\"Cantidad de palabras calculadas en el 30% del vocabulario: {n_tokens_30pct}\")\n",
    "print(f\"Cantidad de palabras reales en el 10% del vocabulario: {n_tokens_10pct_real}\")\n",
    "print(f\"Cantidad de palabras reales en el 20% del vocabulario: {n_tokens_20pct_real}\")\n",
    "print(f\"Cantidad de palabras reales en el 30% del vocabulario: {n_tokens_30pct_real}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
